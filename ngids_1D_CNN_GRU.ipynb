{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "NGIDS_path = './dataset/NGIDS_host_log_1-99.csv'\n",
    "\n",
    "\n",
    "device = torch.device('cuda') # GPU 사용\n",
    "batch_size = 1024\n",
    "slide_window_size = 200\n",
    "learning_rate = 0.001\n",
    "max_epochs = 100\n",
    "input_size = 10\n",
    "hidden_size = 50\n",
    "num_layers = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900542\n"
     ]
    }
   ],
   "source": [
    "NGIDS = pd.read_csv(NGIDS_path)\n",
    "        \n",
    "dropna_NGIDS = NGIDS.dropna(subset=['path', 'sys_call', 'label'])\n",
    "\n",
    "path = np.array(dropna_NGIDS['path'].to_list())\n",
    "syscall = np.array(dropna_NGIDS['sys_call'].to_list())\n",
    "label = np.array(dropna_NGIDS['label'].to_list())\n",
    "\n",
    "ngids_len = int(len(path)/100)\n",
    "\n",
    "print(ngids_len)\n",
    "\n",
    "path = path[:ngids_len]\n",
    "syscall = syscall[:ngids_len]\n",
    "label = label[:ngids_len]\n",
    "\n",
    "def data_split(data, label) :\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.4, random_state=42)\n",
    "    X_vali, X_test, y_vali, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "    \n",
    "    return X_train, y_train, X_vali, y_vali, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_vali, y_vali, X_test, y_test = data_split(list(zip(path, syscall)), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AJOU\\anaconda3\\envs\\torch_semi_supervised\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def save_path(vector_size, window, data_name=\"NGIDS_path_w2v\"):\n",
    "    return \"./dataset/path/\" + f\"vectorsize{vector_size}_window{window}_\" + data_name\n",
    "\n",
    "def save_sys(vector_size, window, data_name = \"NGIDS_vector\"):\n",
    "    return \"./dataset/SystemCall/\" + f\"vectorsize{vector_size}_window{window}_\" + data_name\n",
    "\n",
    "\n",
    "vector_size = 10\n",
    "window = 1\n",
    "\n",
    "NGIDS_sys_model = gensim.models.Word2Vec.load(save_sys(vector_size, window))\n",
    "NGIDS_path_model = gensim.models.Word2Vec.load(save_path(vector_size, window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGIDS_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, label, p2i, s2i, slide_window_size):\n",
    "        self.label = label\n",
    "        self.slide_size = slide_window_size\n",
    "\n",
    "        path_l = []\n",
    "        sys_l = []\n",
    "\n",
    "        for path, sys in data :\n",
    "            path_l.append(p2i[path])\n",
    "            sys_l.append(s2i[sys])\n",
    "\n",
    "        self.data = list(zip(path_l, sys_l))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.slide_size + 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.data[i:i + self.slide_size]).reshape(1, self.slide_size, 2).to(device), torch.tensor(self.label[i:i + self.slide_size], dtype=torch.long).reshape(1, self.slide_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2i = NGIDS_path_model.wv.key_to_index\n",
    "s2i = NGIDS_sys_model.wv.key_to_index\n",
    "NGIDS_dataset = NGIDS_Dataset(X_train, y_train, p2i, s2i, slide_window_size)\n",
    "train_loader = DataLoader(NGIDS_dataset, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, hhidden_size, num_layers, dropout_p=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.gru1 = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.gru2 = nn.GRU(hidden_size, hhidden_size, batch_first=True)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch, _ = self.gru1(batch)\n",
    "        batch = self.dropout(batch)\n",
    "        outputs, hidden = self.gru2(batch)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, hhidden_size, num_layers, dropout_p=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.gru1 = nn.GRU(hhidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.gru2 = nn.GRU(hidden_size, input_size, batch_first=True)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch, _ = self.gru1(batch)\n",
    "        batch = self.dropout(batch)\n",
    "        output, hidden = self.gru2(batch)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, hhidden_size, num_layers, path_vecs, sys_vecs):\n",
    "        super(CNN_AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.path_emb = nn.Embedding.from_pretrained(torch.tensor(path_vecs, dtype=torch.float).cuda(), freeze=True)\n",
    "        self.sys_emb = nn.Embedding.from_pretrained(torch.tensor(sys_vecs, dtype=torch.float).cuda(), freeze=True)\n",
    "\n",
    "        self.encoder = Encoder(input_size=input_size, hidden_size=hidden_size, hhidden_size=hhidden_size, num_layers=num_layers)\n",
    "        self.reconstruct_decoder = Decoder(input_size=input_size, hidden_size=hidden_size, hhidden_size=hhidden_size, num_layers=num_layers)\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch, _  = batch\n",
    "\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        batch_size, sequence_length, _ = batch.size()\n",
    "        vector_size = self.input_size\n",
    "        \n",
    "        path_batch = self.path_emb(batch[:,:,0])\n",
    "        sys_batch = self.sys_emb(batch[:,:,1])\n",
    "\n",
    "        batch = path_batch + sys_batch\n",
    "        batch = batch.reshape(batch_size, sequence_length, vector_size)\n",
    "        \n",
    "        outputs, encoder_hidden = self.encoder(batch)\n",
    "        outputs, decoder_hidden = self.reconstruct_decoder(outputs)\n",
    "        \n",
    "        reconstruct_loss = self.criterion(outputs, batch)\n",
    "\n",
    "        batch = batch.to(\"cpu\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return outputs, reconstruct_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, train_loader):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epochs = tqdm(range(max_epochs))\n",
    "    \n",
    "    count = 0\n",
    "    for epoch in epochs:\n",
    "       \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_iterator = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"training\")\n",
    "\n",
    "        for i, batch_data in train_iterator:\n",
    "            \n",
    "            reconstruct_loss = model(batch_data)\n",
    "\n",
    "            loss = reconstruct_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        torch.save(model, f\"model/Auto_encoder_epoch{epoch}.model\")\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 528/528 [02:42<00:00,  3.25it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.28it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.28it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.28it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.28it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.27it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.26it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.31it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.31it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.27it/s]\n",
      "training: 100%|██████████| 528/528 [02:42<00:00,  3.25it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.28it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.27it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.27it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.27it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.28it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.26it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.27it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.27it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.28it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.27it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.28it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.27it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.28it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.27it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.27it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.28it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.28it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.27it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:41<00:00,  3.28it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.28it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.31it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.31it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.31it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.31it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.31it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.30it/s]\n",
      "training: 100%|██████████| 528/528 [02:40<00:00,  3.29it/s]\n",
      "training: 100%|██████████| 528/528 [02:39<00:00,  3.31it/s]\n",
      "training: 100%|██████████| 528/528 [02:43<00:00,  3.23it/s]\n",
      "training: 100%|██████████| 528/528 [02:56<00:00,  3.00it/s]\n",
      "training: 100%|██████████| 528/528 [02:55<00:00,  3.00it/s]\n",
      "training: 100%|██████████| 528/528 [02:55<00:00,  3.00it/s]\n",
      "training: 100%|██████████| 528/528 [02:55<00:00,  3.01it/s]\n",
      "training: 100%|██████████| 528/528 [02:56<00:00,  3.00it/s]\n",
      "training: 100%|██████████| 528/528 [02:55<00:00,  3.00it/s]\n",
      "training: 100%|██████████| 528/528 [02:56<00:00,  3.00it/s]\n",
      "training: 100%|██████████| 528/528 [02:56<00:00,  2.99it/s]\n",
      "training: 100%|██████████| 528/528 [02:55<00:00,  3.01it/s]\n",
      "training: 100%|██████████| 528/528 [02:55<00:00,  3.01it/s]\n",
      "training: 100%|██████████| 528/528 [02:55<00:00,  3.01it/s]\n",
      "training: 100%|██████████| 528/528 [02:55<00:00,  3.01it/s]\n",
      "training: 100%|██████████| 528/528 [02:55<00:00,  3.01it/s]\n",
      "training: 100%|██████████| 528/528 [02:55<00:00,  3.00it/s]\n",
      "training: 100%|██████████| 528/528 [02:55<00:00,  3.01it/s]\n",
      "training: 100%|██████████| 528/528 [02:56<00:00,  3.00it/s]\n",
      "training: 100%|██████████| 528/528 [02:55<00:00,  3.00it/s]\n",
      "training: 100%|██████████| 528/528 [02:55<00:00,  3.00it/s]\n",
      "training: 100%|██████████| 528/528 [02:56<00:00,  2.99it/s]\n",
      "training: 100%|██████████| 528/528 [02:56<00:00,  2.99it/s]\n",
      "training: 100%|██████████| 528/528 [02:54<00:00,  3.02it/s]\n",
      "training: 100%|██████████| 528/528 [02:53<00:00,  3.04it/s]\n",
      "training: 100%|██████████| 528/528 [02:53<00:00,  3.04it/s]\n",
      "training: 100%|██████████| 528/528 [02:54<00:00,  3.02it/s]\n",
      "training: 100%|██████████| 528/528 [02:54<00:00,  3.03it/s]\n",
      "training: 100%|██████████| 528/528 [02:54<00:00,  3.03it/s]\n",
      "training: 100%|██████████| 528/528 [02:53<00:00,  3.04it/s]\n",
      "training: 100%|██████████| 528/528 [02:53<00:00,  3.04it/s]\n",
      "training: 100%|██████████| 528/528 [02:54<00:00,  3.03it/s]\n",
      "training: 100%|██████████| 528/528 [02:53<00:00,  3.04it/s]\n",
      "training: 100%|██████████| 528/528 [02:54<00:00,  3.02it/s]\n",
      "100%|██████████| 100/100 [4:35:20<00:00, 165.21s/it]\n"
     ]
    }
   ],
   "source": [
    "model = CNN_AutoEncoder(input_size, hidden_size, num_layers, NGIDS_path_model.wv.vectors, NGIDS_sys_model.wv.vectors)\n",
    "model.to(device)\n",
    "\n",
    "model = run(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"AutoEncoder.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d4f5e5333919c7baf0c3b886effb8804f3a4378a893d12116a2d10aaca08022"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
