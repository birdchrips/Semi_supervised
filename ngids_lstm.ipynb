{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "NGIDS_path = './dataset/NGIDS_host_log_1-99.csv'\n",
    "\n",
    "\n",
    "device = torch.device('cuda') # GPU 사용\n",
    "batch_size = 1024\n",
    "slide_window_size = 30\n",
    "learning_rate = 0.001\n",
    "max_epochs = 100\n",
    "input_size = 10\n",
    "hidden_size = 50\n",
    "num_layers = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGIDS = pd.read_csv(NGIDS_path)\n",
    "        \n",
    "dropna_NGIDS = NGIDS.dropna(subset=['path', 'sys_call', 'label'])\n",
    "\n",
    "path = np.array(dropna_NGIDS['path'].to_list())\n",
    "syscall = np.array(dropna_NGIDS['sys_call'].to_list())\n",
    "label = np.array(dropna_NGIDS['label'].to_list())\n",
    "\n",
    "def data_split(data, label) :\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.4, random_state=42)\n",
    "    X_vali, X_test, y_vali, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "    \n",
    "    return X_train, y_train, X_vali, y_vali, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_vali, y_vali, X_test, y_test = data_split(list(zip(path, syscall)), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AJOU\\anaconda3\\envs\\torch_semi_supervised\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def save_path(vector_size, window, data_name=\"NGIDS_path_w2v\"):\n",
    "    return \"./dataset/path/\" + f\"vectorsize{vector_size}_window{window}_\" + data_name\n",
    "\n",
    "def save_sys(vector_size, window, data_name = \"NGIDS_vector\"):\n",
    "    return \"./dataset/SystemCall/\" + f\"vectorsize{vector_size}_window{window}_\" + data_name\n",
    "\n",
    "\n",
    "vector_size = 10\n",
    "window = 1\n",
    "\n",
    "NGIDS_sys_model = gensim.models.Word2Vec.load(save_sys(vector_size, window))\n",
    "NGIDS_path_model = gensim.models.Word2Vec.load(save_path(vector_size, window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGIDS_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, label, p2i, s2i, slide_window_size):\n",
    "        self.label = label\n",
    "        self.slide_size = slide_window_size\n",
    "\n",
    "        path_l = []\n",
    "        sys_l = []\n",
    "\n",
    "        for path, sys in data :\n",
    "            path_l.append(p2i[path])\n",
    "            sys_l.append(s2i[sys])\n",
    "\n",
    "        self.data = list(zip(path_l, sys_l))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.slide_size + 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.data[i:i + self.slide_size]).reshape(1, self.slide_size, 2).to(device), torch.tensor(self.label[i:i + self.slide_size], dtype=torch.long).reshape(1, self.slide_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2i = NGIDS_path_model.wv.key_to_index\n",
    "s2i = NGIDS_sys_model.wv.key_to_index\n",
    "NGIDS_dataset = NGIDS_Dataset(X_train, y_train, p2i, s2i, slide_window_size)\n",
    "train_loader = DataLoader(NGIDS_dataset, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_p=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout = dropout_p)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        outpus, (hidden, cell) = self.lstm(batch)\n",
    "        return (hidden, cell)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_p=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout = dropout_p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, (hidden, cell) = self.lstm(x, hidden)\n",
    "        pred = self.fc(output)\n",
    "        return pred, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, path_vecs, sys_vecs):\n",
    "        super(LSTM_AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.path_emb = nn.Embedding.from_pretrained(torch.tensor(path_vecs, dtype=torch.float).cuda(), freeze=True)\n",
    "        self.sys_emb = nn.Embedding.from_pretrained(torch.tensor(sys_vecs, dtype=torch.float).cuda(), freeze=True)\n",
    "\n",
    "        self.encoder = Encoder(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.reconstruct_decoder = Decoder(input_size=input_size, hidden_size=hidden_size, output_size = input_size, num_layers=num_layers)\n",
    "        #self.predict_decoder = Decoder(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch, _  = batch\n",
    "        batch_size, _, sequence_length, _ = batch.size()\n",
    "        vector_size = self.input_size\n",
    "        \n",
    "        path_batch = self.path_emb(batch[:,:,:,0])\n",
    "        sys_batch = self.sys_emb(batch[:,:,:,1])\n",
    "\n",
    "        batch = path_batch + sys_batch\n",
    "        batch = batch.reshape(batch_size, sequence_length, vector_size)\n",
    "        \n",
    "        encoder_hidden = self.encoder(batch)\n",
    "        \n",
    "        '''predict_output = []\n",
    "        temp_input = torch.zeros((batch_size, 1, vector_size), dtype=torch.float).to(device)\n",
    "        hidden = encoder_hidden\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            temp_input, hidden = self.predict_decoder(temp_input, hidden)\n",
    "            predict_output.append(temp_input)\n",
    "            \n",
    "        predict_output = torch.cat(predict_output, dim=1)\n",
    "        predict_loss = self.criterion(predict_output, trg)'''\n",
    "        \n",
    "        inv_idx = torch.arange(sequence_length - 1, -1, -1).long()\n",
    "\n",
    "        reconstruct_output = []\n",
    "        temp_input = torch.zeros((batch_size, 1, vector_size), dtype=torch.float).to(device)\n",
    "        hidden = encoder_hidden\n",
    "        for t in range(sequence_length):\n",
    "            print(f\"temp_intput : {temp_input.size()}\")\n",
    "            temp_input, hidden = self.reconstruct_decoder(temp_input, hidden)\n",
    "            reconstruct_output.append(temp_input)\n",
    "        reconstruct_output = torch.cat(reconstruct_output, dim=1)\n",
    "        reconstruct_loss = self.criterion(reconstruct_output, batch[:, inv_idx, :])\n",
    "            \n",
    "        return reconstruct_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, train_loader):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epochs = tqdm(range(max_epochs))\n",
    "    \n",
    "    count = 0\n",
    "    for epoch in epochs:\n",
    "       \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_iterator = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"training\")\n",
    "\n",
    "        for i, batch_data in train_iterator:\n",
    "            \n",
    "            reconstruct_loss = model(batch_data)\n",
    "\n",
    "            loss = reconstruct_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'output_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10984/94176107.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM_AutoEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNGIDS_path_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNGIDS_sys_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10984/1135253387.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size, hidden_size, num_layers, path_vecs, sys_vecs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreconstruct_decoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;31m#self.predict_decoder = Decoder(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'output_size'"
     ]
    }
   ],
   "source": [
    "model = LSTM_AutoEncoder(input_size, hidden_size, num_layers, NGIDS_path_model.wv.vectors, NGIDS_sys_model.wv.vectors)\n",
    "model.to(device)\n",
    "\n",
    "model = run(model, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch_semi_supervised')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2398447304b6c99822df0b3d08ddaef8a39ed866a61fdd621cb318ad5f76ce08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
